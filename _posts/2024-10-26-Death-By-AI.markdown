---
layout: post
title:  "Death By AI"
date:   2024-10-26 00:00:22 -0400
categories: jekyll update
---
 
![Image]({{site.baseurl}}/assets/images/mcullogh-pitts.PNG)


Short History of AI
----

 AI stands for Artificial Intelligence. It is humans effort to create intelligence in electronic or silicon frameboards. It carries the name artificial as it is man made and is seperate from the creations of nature. Though the history of AI is thousand years old [1], the modern AI has it roots in 20th century. In 1956, a group of scientist (which included heavy weights in the field as Marvin Minksy, John McCarthy and Claude Shannon) who conceived a new field as "artificial Intelligence." There premise was to create thinking machines and discover the scientific basis of thought, mind and computation process of the brain. Earlier then the Darmouth meeting, was the collaborative work of Warren McCulloch (MIT Professor) and Walter Pitts, which set the basis of the neural network as we presently know of [2]. The MIT researchers were influenced by the ideas of Gottfried Leibniz, who stated that thoughts were an outcome of seperate components which were combined together to create concepts and facts and with the mixture of logical rules, all of human knowlegde can be produced. Further, McCulloch was a doctor by profession and saw the human brain as nothing different then a computing machine. As such all thoughts are computable. The neural network was an outcome of reverse-engineering the working of the brain neurons and synapses and the intriguing quality of neurons to be binary, as it sends or don't send an electrical signal. 
                                               
 In 1969, the Minsky XOR problem was published [3]. It stated that a single-layer perceptron (the simplest neural network) is unable to solve the XOR problem (give true output of the two parameters of an input are similar, else it gives false output). This innocouous problem proved to be devastating to the AI field and it caused the AI- winter to set in. It took decades to realize that a multi-layer perceptron can potentially solve the Minsky's problem. Another theorem that brought resurgence of AI was `Backpropagation algorithm`, which allows the neural weights to be adjusted in relation to the error that is accumulated on the predictions [4]. Faster computing power with the advent of GPU (General Processing Unit) and the emergence of large data sets has allowed the widespread adoption of neural network or machine learning technologies. In my personal experience, the advent of faster computer or online clusters and the open-source data sets has been a game-changer. A recent invention of the `Transformers` have become staple in order to make learning models work faster and better in all AI-applications [5]. 


LLM
---

 In the past three years, we have noticed the rise of LLM (Large Language Models) that are able to understand our language and create accurate responses. LLMs are a sub-field of NLP (Natural Language Processing), which is essentially the study of human language and find computational basis for it. NLP falls under two classes: scientific and statistical. The scientific states that language is not only computation but there is more to the story that we currently can not explain. The unexplainable involves things as: origin of language, the mental process, the mind and the concious aspect. This genre of NLP is spearheaded by folks as Noam Chomsky and alike. On the other hand, we have the statistical group that states all languages are a simple probabilistic and Turing complete which can be categorized as computation methods of the brain. This group is lead by Peter Norvig and his followers [6]. So who is winning? Currently, we see the emergence of statistical models to be prevalent in llm domain. Essentially, the whole neural network can be seen as a probabilistic calculator, to categorize or make predictions on the given input streams. 

We find that the two schools of thought in the llm field work to achieve different goals. The scientific group is to answer why questions, while the statistical group tries to answer the hows. At core, we can say that there are scientific queries and engineering possibilities. Still, the notions of Chomsky shouldn't be ignored and must be taken under consideration if we actually want to understand what language really is. These are the points purposed by Chomsky (though they are explained in my simple language):

1. Language is a tool to create thought. Language is not designed for communication. It is a by-product of language properties. 

2. Language is not only computation but has metaphysical properties that modern science, currently, can not explain. This brings in the notion of the origin of language. Many theories but nothing concrete for the time being.

3. Animals have sound. Humans are the only species that posses the power of language.

4. Computers can imitate to understand and respond to our queries, but that doesn't mean it is a rational agent or concious.

5. The origin of language might possibly have genetic endowmnet or forebearing. It is in our genes to create language. 

6. Language is infinite. We use a set of rules (grammar) and a set of alphabets or characters to create an endless stream of thoughts and ideas.

These are the important points that I picked from the scientific perspective. Statistical models have nothing, even remotely, to answer about such notions. As long it works, it will be adopted. But, as users of chatbots we must understand that the mysteries of language are still prevalent though the bots can write essays, create pdfs and create code in matter of seconds, and look and sound as if talking to a real person. Its an imitation game.



AGI
----






References
----------------

[1] Peter Norvig, Artificial Intelligence: A Modern Approach, 2004

[2] https://nautil.us/the-man-who-tried-to-redeem-the-world-with-logic-235253/

[3] Marvin Minsky and Seymour A. Papert, Perceptrons: An Introduction to Computational Geometry, 1969

[4] Paul J. Werbos,  The Roots of Backpropagation : From Ordered Derivatives to Neural Networks and Political Forecasting, 1994

[5] Ashish Vaswani, Attention Is All You Need, 2017

[6] https://norvig.com/chomsky.html